{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import scipy\n",
    "import tensorflow as tf\n",
    "from keras.layers import Input, Conv2D, Lambda,  Flatten, Dropout,Reshape,UpSampling2D\n",
    "from keras.models import Model, Sequential\n",
    "from keras.regularizers import l2,l1_l2,l1\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import TensorBoard, LearningRateScheduler\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from keras.losses import mean_squared_error,cosine_proximity\n",
    "from keras.utils import Sequence\n",
    "from keras.optimizers import SGD\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q git+https://github.com/tensorflow/docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data- Handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.preprocessing\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "class Data_handler():\n",
    "    \"\"\"Generate batches for FMRI prediction\n",
    "    frames_back - how many video frames to take before FMRI frame\n",
    "    frames_forward - how many video frames to take after FMRI frame\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,matlab_file,test_img_csv ='./images/image_test_id.csv',train_img_csv ='./images/image_training_id.csv',voxel_spacing =3,log = 0):\n",
    "        mat = loadmat(matlab_file)\n",
    "        self.data = mat['dataSet'][:,3:]\n",
    "        self.sample_meta = mat['dataSet'][:,:3]\n",
    "        meta=mat['metaData']\n",
    "\n",
    "\n",
    "        self.meta_keys = list(l[0] for l in meta[0][0][0][0])\n",
    "        self.meta_desc = list(l[0] for l in meta[0][0][1][0])\n",
    "        self.voxel_meta = np.nan_to_num(meta[0][0][2][:,3:])\n",
    "        test_img_df = pd.read_csv(test_img_csv, header=None)\n",
    "        train_img_df =pd.read_csv(train_img_csv, header=None)\n",
    "        self.test_img_id = test_img_df[0].values\n",
    "        self.train_img_id = train_img_df[0].values\n",
    "        self.sample_type = {'train':1 , 'test':2 , 'test_imagine' : 3}\n",
    "        self.voxel_spacing = voxel_spacing\n",
    "\n",
    "        self.log = log\n",
    "\n",
    "    def get_meta_field(self,field = 'DataType'):\n",
    "        index = self.meta_keys.index(field)\n",
    "        if(index <3): # 3 first keys are sample meta\n",
    "            return self.sample_meta[:,index]\n",
    "        else:\n",
    "            return self.voxel_meta[index]\n",
    "\n",
    "\n",
    "    def print_meta_desc(self):\n",
    "        print(self.meta_desc)\n",
    "\n",
    "    def get_labels(self, imag_data = 0,test_run_list = None):\n",
    "#         le = preprocessing.LabelEncoder()\n",
    "\n",
    "        img_ids = self.get_meta_field('Label')\n",
    "        type = self.get_meta_field('DataType')\n",
    "        train = (type == self.sample_type['train'])\n",
    "        test = (type == self.sample_type['test'])\n",
    "        imag = (type == self.sample_type['test_imagine'])\n",
    "\n",
    "        img_ids_train = img_ids[train]\n",
    "        img_ids_test = img_ids[test]\n",
    "        img_ids_imag = img_ids[imag]\n",
    "\n",
    "\n",
    "        train_labels  = []\n",
    "        test_labels  =  []\n",
    "        imag_labels = []\n",
    "        for id in img_ids_test:\n",
    "            idx = (np.abs(id - self.test_img_id)).argmin()\n",
    "            test_labels.append(idx)\n",
    "\n",
    "        for id in img_ids_train:\n",
    "            idx = (np.abs(id - self.train_img_id)).argmin()\n",
    "            train_labels.append(idx)\n",
    "\n",
    "        for id in img_ids_imag:\n",
    "            idx = (np.abs(id - self.test_img_id)).argmin()\n",
    "            imag_labels.append(idx)\n",
    "\n",
    "        if (test_run_list is not None):\n",
    "            run = self.get_meta_field('Run')\n",
    "            test = (self.get_meta_field('DataType') == 2).astype(bool)\n",
    "            run = run[test]\n",
    "\n",
    "            select = np.in1d(run, test_run_list)\n",
    "            test_labels = test_labels[select]\n",
    "\n",
    "        #imag_labels = le.fit_transform(img_ids_imag)\n",
    "        if(imag_data):\n",
    "            return np.array(train_labels), np.array(test_labels), np.array(imag_labels)\n",
    "        else:\n",
    "            return np.array(train_labels),np.array(test_labels)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def get_data(self,normalize =1 ,roi = 'ROI_VC',imag_data = 0,test_run_list = None):   # normalize 0-no, 1- per run , 2- train/test seperatly\n",
    "        type = self.get_meta_field('DataType')\n",
    "        train = (type == self.sample_type['train'])\n",
    "        test = (type == self.sample_type['test'])\n",
    "        test_imag = (type == self.sample_type['test_imagine'])\n",
    "        test_all  = np.logical_or(test,test_imag)\n",
    "\n",
    "        roi_select = self.get_meta_field(roi).astype(bool)\n",
    "        data = self.data[:,roi_select]\n",
    "\n",
    "        if(self.log ==1):\n",
    "            data = np.log(1+np.abs(data))*np.sign(data)\n",
    "\n",
    "\n",
    "        if(normalize==1):\n",
    "\n",
    "            run = self.get_meta_field('Run').astype('int')-1\n",
    "            num_runs = np.max(run)+1\n",
    "            data_norm = np.zeros(data.shape)\n",
    "\n",
    "            for r in range(num_runs):\n",
    "                data_norm[r==run] = sklearn.preprocessing.scale(data[r==run])\n",
    "            train_data = data_norm[train]\n",
    "            test_data  = data_norm[test]\n",
    "            test_all = data_norm[test_all]\n",
    "            test_imag = data_norm[test_imag]\n",
    "\n",
    "        else:\n",
    "            train_data = data[train]\n",
    "            test_data  =  data[test]\n",
    "            if(normalize==2):\n",
    "                train_data = sklearn.preprocessing.scale(train_data)\n",
    "                test_data = sklearn.preprocessing.scale(test_data)\n",
    "\n",
    "\n",
    "        if(self.log ==2):\n",
    "            train_data = np.log(1+np.abs(train_data))*np.sign(train_data)\n",
    "            test_data = np.log(1+np.abs(test_data))*np.sign(test_data)\n",
    "            train_data = sklearn.preprocessing.scale(train_data)\n",
    "            test_data = sklearn.preprocessing.scale(test_data)\n",
    "\n",
    "\n",
    "\n",
    "        test_labels =  self.get_labels()[1]\n",
    "        imag_labels = self.get_labels(1)[2]\n",
    "        num_labels = max(test_labels)+1\n",
    "        test_data_avg = np.zeros([num_labels,test_data.shape[1]])\n",
    "        test_imag_avg = np.zeros([num_labels,test_data.shape[1]])\n",
    "\n",
    "        if(test_run_list is not None):\n",
    "            run = self.get_meta_field('Run')\n",
    "            test = (self.get_meta_field('DataType') == 2).astype(bool)\n",
    "            run = run[test]\n",
    "\n",
    "            select = np.in1d(run, test_run_list)\n",
    "            test_data = test_data[select,:]\n",
    "            test_labels = test_labels[select]\n",
    "\n",
    "        for i in range(num_labels):\n",
    "            test_data_avg[i] = np.mean(test_data[test_labels==i],axis=0)\n",
    "            test_imag_avg[i] = np.mean(test_imag[imag_labels == i], axis=0)\n",
    "        if(imag_data):\n",
    "            return train_data, test_data, test_data_avg,test_imag,test_imag_avg\n",
    "\n",
    "        else:\n",
    "            return train_data, test_data, test_data_avg\n",
    "\n",
    "    def get_voxel_loc(self):\n",
    "        x = self.get_meta_field('voxel_x')\n",
    "        y = self.get_meta_field('voxel_y')\n",
    "        z = self.get_meta_field('voxel_z')\n",
    "        dim = [int(x.max() -x.min()+1),int(y.max() -y.min()+1), int(z.max() -z.min()+1)]\n",
    "        return [x,y,z] , dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "handler=Data_handler(matlab_file ='./Subject3.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y,Y_test,Y_test_avg = handler.get_data(roi = 'ROI_VC')\n",
    "labels_train, labels = handler.get_labels()\n",
    "NUM_VOXELS = Y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4643"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_VOXELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "file= np.load('images_npz.npz')\n",
    "X = file['train_images']\n",
    "X_test = file['test_images']\n",
    "\n",
    "X= X[labels_train]\n",
    "X_test_sorted = X_test\n",
    "X_test = X_test[labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Losses and Schedules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_lrate = 0.1\n",
    "\n",
    "\n",
    "def step_decay(epoch):\n",
    "    \n",
    "    lrate = 0.1 #Learning Rate\n",
    "    \n",
    "    # So for various epochs we will be using various learning rates\n",
    "    \n",
    "    if(epoch>20):\n",
    "        lrate = 0.01\n",
    "    if (epoch > 35):\n",
    "        lrate = 0.001\n",
    "    if (epoch > 45):\n",
    "        lrate = 0.0001\n",
    "    if (epoch > 50):\n",
    "        lrate = 0.00001\n",
    "\n",
    "    return lrate\n",
    "\n",
    "def combined_loss(y_true, y_pred):\n",
    "    return mean_squared_error(y_true, y_pred)+ 0.1*cosine_proximity(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.regularizers import Regularizer\n",
    "from keras.regularizers import l2,l1_l2,l1\n",
    "\n",
    "class GroupLasso(Regularizer):\n",
    "    \"\"\" GroupLasso Regularizer\n",
    "    Used for regularization of fc layer, from conv activations to dense activations\n",
    "    Assumes that input weight x is formatted the following way:\n",
    "    height, width, channels, output units\n",
    "    channels in the same position will be grouped together (for each output unit separately)\n",
    "     Arguments\n",
    "        l1: Float; L1 regularization factor.\n",
    "        gl: Float; gl group regularization factor.\n",
    "        gl_n: first order neighbor's contribution to group regularization\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, l1=0., gl=0.,gl_n=0):\n",
    "        self.l1 = K.cast_to_floatx(l1)\n",
    "        self.gl = K.cast_to_floatx(gl)\n",
    "        self.gl_n = K.cast_to_floatx(gl_n)\n",
    "\n",
    "\n",
    "    def __call__(self, x):\n",
    "\n",
    "        regularization = self.l1 * K.mean(K.abs(x))\n",
    "        x_sq = K.square(x)\n",
    "\n",
    "        if (self.gl_n > 0):\n",
    "            x_sq_pad = tf.pad(x_sq, [[1, 1], [1, 1], [0, 0], [0, 0]], \"SYMMETRIC\")\n",
    "            x_sq_avg_n = (x_sq_pad[:-2, 1:-1] + x_sq_pad[2:, 1:-1] + x_sq_pad[1:-1, :-2] + x_sq_pad[1:-1, 2:]) / 4\n",
    "            x_sq = (x_sq + self.gl_n * x_sq_avg_n) / (1 + self.gl_n)\n",
    "        regularization += self.gl * K.mean(K.sqrt(K.mean(x_sq, axis=-2)))  # assumes weight structure x,y,ch,voxel\n",
    "        return regularization\n",
    "\n",
    "\n",
    "    def get_config(self):\n",
    "        return {'l1': float(self.l1), 'l2': float(self.gl)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "MEAN_PIXELS = [123.68, 116.779, 103.939]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subtract_mean(x):\n",
    "    mean = tf.constant(MEAN_PIXELS,shape=[1,1,1,3],dtype=tf.float32)\n",
    "    return tf.subtract(x,mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _weights(net_layers, layer, expected_layer_name):\n",
    "    \"\"\" Return the weights and biases trained by VGG\n",
    "    \"\"\"\n",
    "    W = net_layers[0][layer][0][0][2][0][0]\n",
    "    b = net_layers[0][layer][0][0][2][0][1]\n",
    "    layer_name = net_layers[0][layer][0][0][0][0]\n",
    "    assert layer_name == expected_layer_name\n",
    "    return W, b.reshape(b.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_prod(l):\n",
    "    prod = 1\n",
    "    for e in l:\n",
    "        prod*=e\n",
    "    return prod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d_relu_(prev_layer,net_layers, layer, layer_name,stride=1, pad=None):\n",
    "    \"\"\" Return the Conv2D layer with RELU using the weights, biases from the VGG\n",
    "    model at 'layer'.\n",
    "    Inputs:\n",
    "        net_layers: holding all the layers of VGGNet\n",
    "        prev_layer: the output tensor from the previous layer\n",
    "        layer: the index to current layer in net_layers\n",
    "        layer_name: the string that is the name of the current layer.\n",
    "                    It's used to specify variable_scope.\n",
    "    Output:\n",
    "        relu applied on the convolution.\n",
    "    \"\"\"\n",
    "    W, b = _weights(net_layers, layer, layer_name)\n",
    "    W = tf.constant(W, name='weights')\n",
    "    b = tf.constant(b, name='bias')\n",
    "\n",
    "    conv2d = tf.nn.conv2d(prev_layer,W, strides=[1, stride, stride, 1], padding='SAME')\n",
    "    return tf.nn.relu(conv2d + b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dense_c2f_gl(Layer):\n",
    "\n",
    "    def __init__(self, units=1024,l1=0.1,gl=0.1,gl_n=0,kernel_init = \"glorot_normal\", **kwargs):\n",
    "        self.units = units\n",
    "        self.l1 = l1\n",
    "        self.gl = gl\n",
    "        self.gl_n = gl_n\n",
    "        self.kernel_init =kernel_init\n",
    "        super(dense_c2f_gl, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Create a trainable weight variable for this layer.\n",
    "        self.s = input_shape\n",
    "        shape = list(input_shape[1:])+[self.units]\n",
    "        self.kernel = self.add_weight(name='dense_c2f_gl_kernel',\n",
    "                                      shape=shape,\n",
    "                                      regularizer= GroupLasso(l1=self.l1,gl=self.gl,gl_n= self.gl_n) ,\n",
    "\n",
    "                                      initializer=self.kernel_init,\n",
    "                                      trainable=True)\n",
    "        self.bias = self.add_weight(name='dense_c2f_gl_bias', shape=(self.units,),initializer=\"glorot_normal\",trainable=True)\n",
    "\n",
    "        super(dense_c2f_gl, self).build(input_shape)  # Be sure to call this at the end\n",
    "\n",
    "    def call(self, x):\n",
    "        input_shape = self.s\n",
    "        size =  list_prod(input_shape[1:])\n",
    "        x = tf.reshape(x, [-1, size])\n",
    "        w = tf.reshape(self.kernel,[size ,self.units])\n",
    "        output = K.dot(x, w)\n",
    "\n",
    "        output = K.bias_add(output, self.bias)\n",
    "        return output\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class encoder_param():\n",
    "    def __init__(self,num_voxels):\n",
    "        self.num_voxels = num_voxels\n",
    "        self.resolution = 112\n",
    "        self.conv_l1_reg = 1e-5\n",
    "        self.conv_l2_reg = 0.001\n",
    "        self.fc_reg_l1 = 10\n",
    "        self.fc_reg_gl = 800\n",
    "        self.fc_reg_gl_n = 0.5\n",
    "        self.conv_ch = 32\n",
    "        self.num_conv_layers = 2\n",
    "        self.conv1_stride = 2\n",
    "        self.conv_last_dim  =14\n",
    "        self.drop_out = 0.5\n",
    "\n",
    "        self.caffenet_models_weights=scipy.io.loadmat('imagenet-caffe-ref.mat') #Pretrained Alexnet network weights\n",
    "        self.caffenet_models_weights=self.caffenet_models_weights['layers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(param,name = 'encoder'):\n",
    "    input_shape = (param.resolution, param.resolution, 3)\n",
    "    model = Sequential(name =name)\n",
    "    model.add(Lambda(lambda img: img[:, :, :, ::-1] * 255.0, input_shape=input_shape))\n",
    "    model.add(Lambda(subtract_mean))\n",
    "    model.add(Lambda(conv2d_relu_,\n",
    "                            arguments={'net_layers': param.caffenet_models_weights, 'layer': 0, 'layer_name': 'conv1', 'stride': param.conv1_stride,\n",
    "                                       'pad': 'SAME'}))\n",
    "    model.add(BatchNormalization(axis=-1))\n",
    "    for i in range(param.num_conv_layers):\n",
    "        model.add(Conv2D(param.conv_ch, (3, 3), padding='same', kernel_initializer=\"glorot_normal\", activation='relu',\n",
    "                                kernel_regularizer=l1_l2(param.conv_l1_reg,param.conv_l2_reg), strides=(2, 2)))\n",
    "\n",
    "        model.add(BatchNormalization(axis=-1))\n",
    "\n",
    "    model.add(Flatten())  # flatten needed for dropout, without it suboptimal results are observed\n",
    "    model.add(Dropout(param.drop_out))\n",
    "\n",
    "    model.add(Reshape((param.conv_last_dim, param.conv_last_dim, param.conv_ch)))\n",
    "    model.add(dense_c2f_gl(units=param.num_voxels, l1=param.fc_reg_l1, gl=param.fc_reg_gl, gl_n=param.fc_reg_gl_n))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lambda_1 (Lambda)            (None, 112, 112, 3)       0         \n",
      "_________________________________________________________________\n",
      "lambda_2 (Lambda)            (None, 112, 112, 3)       0         \n",
      "_________________________________________________________________\n",
      "lambda_3 (Lambda)            (None, 56, 56, 96)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 56, 56, 96)        384       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 28, 28, 32)        27680     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 28, 28, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 14, 14, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 14, 14, 32)        128       \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "dense_c2f_gl_1 (dense_c2f_gl (None, 4643)              29125539  \n",
      "=================================================================\n",
      "Total params: 29,163,107\n",
      "Trainable params: 29,162,787\n",
      "Non-trainable params: 320\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "enc_param = encoder_param(NUM_VOXELS)\n",
    "enc_param.drop_out = 0.5\n",
    "\n",
    "vision_model = encoder(enc_param)\n",
    "vision_model.compile(loss=combined_loss, optimizer= SGD(lr=initial_lrate,decay = 0.0 , momentum = 0.9,nesterov=True),metrics=['mse','cosine_proximity','mae'])\n",
    "print(vision_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CallBacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callbacks = []\n",
    "# log_path='./logs'\n",
    "\n",
    "# # log_path = config_file.encoder_tenosrboard_logs\n",
    "# tb_callback = TensorBoard(log_path)\n",
    "# tb_callback.set_model(vision_model)\n",
    "# callbacks.append(tb_callback)\n",
    "callbacks=[]\n",
    "reduce_lr = LearningRateScheduler(step_decay)\n",
    "callbacks.append(reduce_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Save the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import shift\n",
    "def rand_shift(img,max_shift = 0 ):\n",
    "    \"\"\"\n",
    "    randomly shifted image\n",
    "    :param img: image\n",
    "    :param max_shift: image output size\n",
    "    :return randomly shifted image\n",
    "    \"\"\"\n",
    "    x_shift, y_shift = np.random.randint(-max_shift, max_shift + 1, size=2)\n",
    "    img_shifted = shift(img,[x_shift, y_shift, 0], prefilter=False, order=0, mode='nearest')\n",
    "    return img_shifted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class batch_generator_dec(Sequence):\n",
    "\n",
    "    \"\"\"\n",
    "    Generates batches for decoder model\n",
    "    :param X: images\n",
    "    :param Y: fMRI activations\n",
    "    (X,Y correspond )\n",
    "    :param batch_size: batch_size\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,  X, Y, batch_size =32):\n",
    "        self.indexes = {}\n",
    "        self.batch_size = batch_size\n",
    "        self.Y = Y\n",
    "        self.X = X\n",
    "        self.indexes  = np.random.permutation(self.Y.shape[0])\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return max(int(self.Y.shape[0] // self.batch_size), 1) #\n",
    "    def __getitem__(self,batch_num):\n",
    "        indexes = (self.indexes)[batch_num * self.batch_size:(batch_num + 1) * self.batch_size]\n",
    "        return self.Y[indexes], self.X[indexes]\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.random.permutation(self.indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class batch_generator_enc(batch_generator_dec):\n",
    "    \"\"\"\n",
    "    Generates batches for encoder model\n",
    "    :param X: images\n",
    "    :param Y: fMRI activations\n",
    "    (X,Y correspond )\n",
    "    :param batch_size: batch_size\n",
    "    :param max_shift: max random shift applied on images\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, X, Y, batch_size=32,max_shift = 5):\n",
    "        super().__init__(X, Y, batch_size)\n",
    "        self.max_shift = max_shift\n",
    "\n",
    "    def __getitem__(self,batch_num):\n",
    "        y, x = super().__getitem__(batch_num)\n",
    "        x_shifted = np.zeros(x.shape)\n",
    "        for i in range(x.shape[0]):\n",
    "            x_shifted[i] = rand_shift(x[i],self.max_shift)\n",
    "        return x_shifted,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = batch_generator_enc(X, Y, batch_size=64,max_shift = 5)\n",
    "test_generator = batch_generator_enc(X_test_sorted, Y_test_avg, batch_size=50,max_shift = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      " - 87s - loss: 2.1823 - mse: 0.9939 - cosine_proximity: 0.0368 - mae: 0.7874 - val_loss: 1.1972 - val_mse: 0.1636 - val_cosine_proximity: 0.1318 - val_mae: 0.2991\n",
      "Epoch 2/80\n",
      " - 78s - loss: 1.8213 - mse: 0.9460 - cosine_proximity: 0.1669 - mae: 0.7672 - val_loss: 0.8290 - val_mse: 0.1261 - val_cosine_proximity: 0.1539 - val_mae: 0.2657\n",
      "Epoch 3/80\n",
      " - 74s - loss: 1.4765 - mse: 0.9219 - cosine_proximity: 0.2345 - mae: 0.7571 - val_loss: 0.5042 - val_mse: 0.0899 - val_cosine_proximity: 0.2165 - val_mae: 0.2257\n",
      "Epoch 4/80\n",
      " - 69s - loss: 1.2464 - mse: 0.9219 - cosine_proximity: 0.2541 - mae: 0.7572 - val_loss: 0.3407 - val_mse: 0.0844 - val_cosine_proximity: 0.2444 - val_mae: 0.2193\n",
      "Epoch 5/80\n",
      " - 70s - loss: 1.1637 - mse: 0.9308 - cosine_proximity: 0.2175 - mae: 0.7610 - val_loss: 0.2812 - val_mse: 0.0791 - val_cosine_proximity: 0.2873 - val_mae: 0.2125\n",
      "Epoch 6/80\n",
      " - 69s - loss: 1.1382 - mse: 0.9345 - cosine_proximity: 0.2025 - mae: 0.7625 - val_loss: 0.2612 - val_mse: 0.0780 - val_cosine_proximity: 0.3110 - val_mae: 0.2125\n",
      "Epoch 7/80\n",
      " - 69s - loss: 1.1244 - mse: 0.9326 - cosine_proximity: 0.2048 - mae: 0.7616 - val_loss: 0.2516 - val_mse: 0.0770 - val_cosine_proximity: 0.3234 - val_mae: 0.2113\n",
      "Epoch 8/80\n",
      " - 70s - loss: 1.1173 - mse: 0.9329 - cosine_proximity: 0.2064 - mae: 0.7616 - val_loss: 0.2392 - val_mse: 0.0740 - val_cosine_proximity: 0.3507 - val_mae: 0.2072\n",
      "Epoch 9/80\n",
      " - 69s - loss: 1.1120 - mse: 0.9317 - cosine_proximity: 0.2064 - mae: 0.7611 - val_loss: 0.2315 - val_mse: 0.0724 - val_cosine_proximity: 0.3679 - val_mae: 0.2053\n",
      "Epoch 10/80\n",
      " - 70s - loss: 1.1068 - mse: 0.9306 - cosine_proximity: 0.2112 - mae: 0.7605 - val_loss: 0.2302 - val_mse: 0.0723 - val_cosine_proximity: 0.3759 - val_mae: 0.2062\n",
      "Epoch 11/80\n",
      " - 68s - loss: 1.1077 - mse: 0.9329 - cosine_proximity: 0.2098 - mae: 0.7615 - val_loss: 0.2272 - val_mse: 0.0724 - val_cosine_proximity: 0.3788 - val_mae: 0.2067\n",
      "Epoch 12/80\n",
      " - 67s - loss: 1.1004 - mse: 0.9288 - cosine_proximity: 0.2131 - mae: 0.7597 - val_loss: 0.2248 - val_mse: 0.0722 - val_cosine_proximity: 0.3802 - val_mae: 0.2057\n",
      "Epoch 13/80\n",
      " - 66s - loss: 1.1031 - mse: 0.9320 - cosine_proximity: 0.2124 - mae: 0.7609 - val_loss: 0.2229 - val_mse: 0.0716 - val_cosine_proximity: 0.3853 - val_mae: 0.2043\n",
      "Epoch 14/80\n",
      " - 70s - loss: 1.0966 - mse: 0.9270 - cosine_proximity: 0.2195 - mae: 0.7590 - val_loss: 0.2230 - val_mse: 0.0720 - val_cosine_proximity: 0.3792 - val_mae: 0.2051\n",
      "Epoch 15/80\n",
      " - 84s - loss: 1.1004 - mse: 0.9320 - cosine_proximity: 0.2185 - mae: 0.7611 - val_loss: 0.2229 - val_mse: 0.0722 - val_cosine_proximity: 0.3798 - val_mae: 0.2057\n",
      "Epoch 16/80\n",
      " - 76s - loss: 1.0957 - mse: 0.9266 - cosine_proximity: 0.2179 - mae: 0.7589 - val_loss: 0.2237 - val_mse: 0.0717 - val_cosine_proximity: 0.3858 - val_mae: 0.2053\n",
      "Epoch 17/80\n",
      " - 72s - loss: 1.0924 - mse: 0.9229 - cosine_proximity: 0.2233 - mae: 0.7572 - val_loss: 0.2183 - val_mse: 0.0697 - val_cosine_proximity: 0.4058 - val_mae: 0.2029\n",
      "Epoch 18/80\n",
      " - 78s - loss: 1.0922 - mse: 0.9234 - cosine_proximity: 0.2228 - mae: 0.7575 - val_loss: 0.2189 - val_mse: 0.0699 - val_cosine_proximity: 0.4003 - val_mae: 0.2021\n",
      "Epoch 19/80\n",
      " - 89s - loss: 1.0950 - mse: 0.9249 - cosine_proximity: 0.2274 - mae: 0.7582 - val_loss: 0.2225 - val_mse: 0.0706 - val_cosine_proximity: 0.4005 - val_mae: 0.2028\n",
      "Epoch 20/80\n",
      " - 68s - loss: 1.0931 - mse: 0.9216 - cosine_proximity: 0.2305 - mae: 0.7568 - val_loss: 0.2247 - val_mse: 0.0716 - val_cosine_proximity: 0.3895 - val_mae: 0.2042\n",
      "Epoch 21/80\n",
      " - 77s - loss: 1.0947 - mse: 0.9235 - cosine_proximity: 0.2340 - mae: 0.7574 - val_loss: 0.2210 - val_mse: 0.0688 - val_cosine_proximity: 0.4189 - val_mae: 0.2011\n",
      "Epoch 22/80\n",
      " - 98s - loss: 1.1009 - mse: 0.9104 - cosine_proximity: 0.2577 - mae: 0.7523 - val_loss: 0.2637 - val_mse: 0.0721 - val_cosine_proximity: 0.3811 - val_mae: 0.2066\n",
      "Epoch 23/80\n",
      " - 90s - loss: 1.1063 - mse: 0.9097 - cosine_proximity: 0.2714 - mae: 0.7518 - val_loss: 0.2408 - val_mse: 0.0692 - val_cosine_proximity: 0.4156 - val_mae: 0.2018\n",
      "Epoch 24/80\n",
      " - 88s - loss: 1.0717 - mse: 0.8998 - cosine_proximity: 0.2912 - mae: 0.7477 - val_loss: 0.2112 - val_mse: 0.0675 - val_cosine_proximity: 0.4362 - val_mae: 0.1989\n",
      "Epoch 25/80\n",
      " - 88s - loss: 1.0448 - mse: 0.8990 - cosine_proximity: 0.3014 - mae: 0.7475 - val_loss: 0.1860 - val_mse: 0.0669 - val_cosine_proximity: 0.4446 - val_mae: 0.1979\n",
      "Epoch 26/80\n",
      " - 73s - loss: 1.0201 - mse: 0.8971 - cosine_proximity: 0.3053 - mae: 0.7468 - val_loss: 0.1640 - val_mse: 0.0663 - val_cosine_proximity: 0.4535 - val_mae: 0.1969\n",
      "Epoch 27/80\n",
      " - 73s - loss: 1.0072 - mse: 0.9026 - cosine_proximity: 0.3042 - mae: 0.7489 - val_loss: 0.1468 - val_mse: 0.0660 - val_cosine_proximity: 0.4582 - val_mae: 0.1965\n",
      "Epoch 28/80\n",
      " - 70s - loss: 0.9975 - mse: 0.9069 - cosine_proximity: 0.2953 - mae: 0.7507 - val_loss: 0.1331 - val_mse: 0.0658 - val_cosine_proximity: 0.4607 - val_mae: 0.1962\n",
      "Epoch 29/80\n",
      " - 90s - loss: 0.9922 - mse: 0.9124 - cosine_proximity: 0.2880 - mae: 0.7530 - val_loss: 0.1227 - val_mse: 0.0656 - val_cosine_proximity: 0.4630 - val_mae: 0.1959\n",
      "Epoch 30/80\n",
      " - 89s - loss: 0.9826 - mse: 0.9113 - cosine_proximity: 0.2818 - mae: 0.7526 - val_loss: 0.1139 - val_mse: 0.0653 - val_cosine_proximity: 0.4678 - val_mae: 0.1954\n",
      "Epoch 31/80\n",
      " - 99s - loss: 0.9822 - mse: 0.9169 - cosine_proximity: 0.2726 - mae: 0.7549 - val_loss: 0.1076 - val_mse: 0.0651 - val_cosine_proximity: 0.4702 - val_mae: 0.1951\n",
      "Epoch 32/80\n",
      " - 105s - loss: 0.9783 - mse: 0.9170 - cosine_proximity: 0.2655 - mae: 0.7550 - val_loss: 0.1035 - val_mse: 0.0651 - val_cosine_proximity: 0.4708 - val_mae: 0.1952\n",
      "Epoch 33/80\n",
      " - 87s - loss: 0.9796 - mse: 0.9216 - cosine_proximity: 0.2597 - mae: 0.7568 - val_loss: 0.0996 - val_mse: 0.0649 - val_cosine_proximity: 0.4727 - val_mae: 0.1948\n",
      "Epoch 34/80\n",
      " - 86s - loss: 0.9766 - mse: 0.9214 - cosine_proximity: 0.2575 - mae: 0.7567 - val_loss: 0.0968 - val_mse: 0.0648 - val_cosine_proximity: 0.4743 - val_mae: 0.1947\n",
      "Epoch 35/80\n",
      " - 88s - loss: 0.9772 - mse: 0.9238 - cosine_proximity: 0.2543 - mae: 0.7576 - val_loss: 0.0954 - val_mse: 0.0649 - val_cosine_proximity: 0.4725 - val_mae: 0.1951\n",
      "Epoch 36/80\n",
      " - 98s - loss: 0.9712 - mse: 0.9193 - cosine_proximity: 0.2507 - mae: 0.7558 - val_loss: 0.0931 - val_mse: 0.0647 - val_cosine_proximity: 0.4746 - val_mae: 0.1946\n",
      "Epoch 37/80\n",
      " - 105s - loss: 0.9696 - mse: 0.9184 - cosine_proximity: 0.2560 - mae: 0.7556 - val_loss: 0.0940 - val_mse: 0.0643 - val_cosine_proximity: 0.4781 - val_mae: 0.1942\n",
      "Epoch 38/80\n",
      " - 99s - loss: 0.9695 - mse: 0.9180 - cosine_proximity: 0.2555 - mae: 0.7552 - val_loss: 0.0926 - val_mse: 0.0642 - val_cosine_proximity: 0.4801 - val_mae: 0.1940\n",
      "Epoch 39/80\n",
      " - 98s - loss: 0.9702 - mse: 0.9204 - cosine_proximity: 0.2583 - mae: 0.7562 - val_loss: 0.0907 - val_mse: 0.0641 - val_cosine_proximity: 0.4813 - val_mae: 0.1939\n",
      "Epoch 40/80\n",
      " - 118s - loss: 0.9686 - mse: 0.9203 - cosine_proximity: 0.2567 - mae: 0.7562 - val_loss: 0.0889 - val_mse: 0.0640 - val_cosine_proximity: 0.4821 - val_mae: 0.1938\n",
      "Epoch 41/80\n",
      " - 113s - loss: 0.9654 - mse: 0.9187 - cosine_proximity: 0.2568 - mae: 0.7556 - val_loss: 0.0874 - val_mse: 0.0640 - val_cosine_proximity: 0.4821 - val_mae: 0.1938\n",
      "Epoch 42/80\n",
      " - 98s - loss: 0.9671 - mse: 0.9219 - cosine_proximity: 0.2586 - mae: 0.7567 - val_loss: 0.0861 - val_mse: 0.0640 - val_cosine_proximity: 0.4821 - val_mae: 0.1938\n",
      "Epoch 43/80\n",
      " - 100s - loss: 0.9655 - mse: 0.9213 - cosine_proximity: 0.2554 - mae: 0.7565 - val_loss: 0.0848 - val_mse: 0.0639 - val_cosine_proximity: 0.4828 - val_mae: 0.1937\n",
      "Epoch 44/80\n",
      " - 101s - loss: 0.9639 - mse: 0.9210 - cosine_proximity: 0.2566 - mae: 0.7564 - val_loss: 0.0837 - val_mse: 0.0639 - val_cosine_proximity: 0.4828 - val_mae: 0.1937\n",
      "Epoch 45/80\n",
      " - 126s - loss: 0.9623 - mse: 0.9201 - cosine_proximity: 0.2538 - mae: 0.7562 - val_loss: 0.0826 - val_mse: 0.0639 - val_cosine_proximity: 0.4832 - val_mae: 0.1937\n",
      "Epoch 46/80\n",
      " - 154s - loss: 0.9639 - mse: 0.9225 - cosine_proximity: 0.2527 - mae: 0.7571 - val_loss: 0.0817 - val_mse: 0.0638 - val_cosine_proximity: 0.4835 - val_mae: 0.1936\n",
      "Epoch 47/80\n",
      " - 133s - loss: 0.9589 - mse: 0.9182 - cosine_proximity: 0.2526 - mae: 0.7553 - val_loss: 0.0813 - val_mse: 0.0638 - val_cosine_proximity: 0.4835 - val_mae: 0.1936\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/80\n",
      " - 142s - loss: 0.9632 - mse: 0.9228 - cosine_proximity: 0.2541 - mae: 0.7572 - val_loss: 0.0812 - val_mse: 0.0638 - val_cosine_proximity: 0.4838 - val_mae: 0.1936\n",
      "Epoch 49/80\n",
      " - 113s - loss: 0.9633 - mse: 0.9230 - cosine_proximity: 0.2535 - mae: 0.7572 - val_loss: 0.0810 - val_mse: 0.0638 - val_cosine_proximity: 0.4840 - val_mae: 0.1936\n",
      "Epoch 50/80\n",
      " - 123s - loss: 0.9582 - mse: 0.9182 - cosine_proximity: 0.2551 - mae: 0.7554 - val_loss: 0.0808 - val_mse: 0.0638 - val_cosine_proximity: 0.4843 - val_mae: 0.1935\n",
      "Epoch 51/80\n",
      " - 107s - loss: 0.9618 - mse: 0.9218 - cosine_proximity: 0.2537 - mae: 0.7569 - val_loss: 0.0806 - val_mse: 0.0637 - val_cosine_proximity: 0.4844 - val_mae: 0.1935\n",
      "Epoch 52/80\n",
      " - 107s - loss: 0.9591 - mse: 0.9191 - cosine_proximity: 0.2541 - mae: 0.7560 - val_loss: 0.0806 - val_mse: 0.0637 - val_cosine_proximity: 0.4844 - val_mae: 0.1935\n",
      "Epoch 53/80\n",
      " - 108s - loss: 0.9629 - mse: 0.9230 - cosine_proximity: 0.2537 - mae: 0.7573 - val_loss: 0.0806 - val_mse: 0.0637 - val_cosine_proximity: 0.4846 - val_mae: 0.1935\n",
      "Epoch 54/80\n",
      " - 114s - loss: 0.9656 - mse: 0.9256 - cosine_proximity: 0.2528 - mae: 0.7584 - val_loss: 0.0805 - val_mse: 0.0637 - val_cosine_proximity: 0.4847 - val_mae: 0.1935\n",
      "Epoch 55/80\n",
      " - 109s - loss: 0.9614 - mse: 0.9214 - cosine_proximity: 0.2530 - mae: 0.7567 - val_loss: 0.0805 - val_mse: 0.0637 - val_cosine_proximity: 0.4848 - val_mae: 0.1935\n",
      "Epoch 56/80\n",
      " - 105s - loss: 0.9612 - mse: 0.9212 - cosine_proximity: 0.2524 - mae: 0.7565 - val_loss: 0.0805 - val_mse: 0.0637 - val_cosine_proximity: 0.4849 - val_mae: 0.1935\n",
      "Epoch 57/80\n",
      " - 113s - loss: 0.9630 - mse: 0.9231 - cosine_proximity: 0.2531 - mae: 0.7573 - val_loss: 0.0804 - val_mse: 0.0637 - val_cosine_proximity: 0.4850 - val_mae: 0.1934\n",
      "Epoch 58/80\n",
      " - 131s - loss: 0.9619 - mse: 0.9222 - cosine_proximity: 0.2553 - mae: 0.7568 - val_loss: 0.0804 - val_mse: 0.0637 - val_cosine_proximity: 0.4850 - val_mae: 0.1934\n",
      "Epoch 59/80\n",
      " - 150s - loss: 0.9598 - mse: 0.9200 - cosine_proximity: 0.2542 - mae: 0.7561 - val_loss: 0.0804 - val_mse: 0.0637 - val_cosine_proximity: 0.4850 - val_mae: 0.1934\n",
      "Epoch 60/80\n",
      " - 138s - loss: 0.9614 - mse: 0.9215 - cosine_proximity: 0.2536 - mae: 0.7569 - val_loss: 0.0804 - val_mse: 0.0637 - val_cosine_proximity: 0.4851 - val_mae: 0.1934\n",
      "Epoch 61/80\n",
      " - 107s - loss: 0.9605 - mse: 0.9207 - cosine_proximity: 0.2543 - mae: 0.7563 - val_loss: 0.0803 - val_mse: 0.0637 - val_cosine_proximity: 0.4852 - val_mae: 0.1934\n",
      "Epoch 62/80\n",
      " - 61s - loss: 0.9615 - mse: 0.9219 - cosine_proximity: 0.2553 - mae: 0.7568 - val_loss: 0.0803 - val_mse: 0.0637 - val_cosine_proximity: 0.4851 - val_mae: 0.1934\n",
      "Epoch 63/80\n",
      " - 65s - loss: 0.9604 - mse: 0.9205 - cosine_proximity: 0.2532 - mae: 0.7561 - val_loss: 0.0803 - val_mse: 0.0637 - val_cosine_proximity: 0.4851 - val_mae: 0.1934\n",
      "Epoch 64/80\n",
      " - 84s - loss: 0.9648 - mse: 0.9250 - cosine_proximity: 0.2536 - mae: 0.7582 - val_loss: 0.0803 - val_mse: 0.0637 - val_cosine_proximity: 0.4850 - val_mae: 0.1934\n",
      "Epoch 65/80\n",
      " - 56s - loss: 0.9605 - mse: 0.9209 - cosine_proximity: 0.2548 - mae: 0.7564 - val_loss: 0.0803 - val_mse: 0.0637 - val_cosine_proximity: 0.4851 - val_mae: 0.1934\n",
      "Epoch 66/80\n",
      " - 56s - loss: 0.9639 - mse: 0.9241 - cosine_proximity: 0.2528 - mae: 0.7578 - val_loss: 0.0803 - val_mse: 0.0637 - val_cosine_proximity: 0.4851 - val_mae: 0.1934\n",
      "Epoch 67/80\n",
      " - 54s - loss: 0.9609 - mse: 0.9212 - cosine_proximity: 0.2536 - mae: 0.7566 - val_loss: 0.0803 - val_mse: 0.0637 - val_cosine_proximity: 0.4852 - val_mae: 0.1934\n",
      "Epoch 68/80\n",
      " - 1830s - loss: 0.9587 - mse: 0.9188 - cosine_proximity: 0.2522 - mae: 0.7555 - val_loss: 0.0802 - val_mse: 0.0637 - val_cosine_proximity: 0.4852 - val_mae: 0.1934\n",
      "Epoch 69/80\n",
      " - 4509s - loss: 0.9594 - mse: 0.9198 - cosine_proximity: 0.2553 - mae: 0.7559 - val_loss: 0.0802 - val_mse: 0.0637 - val_cosine_proximity: 0.4852 - val_mae: 0.1934\n",
      "Epoch 70/80\n",
      " - 7469s - loss: 0.9621 - mse: 0.9224 - cosine_proximity: 0.2536 - mae: 0.7571 - val_loss: 0.0802 - val_mse: 0.0637 - val_cosine_proximity: 0.4851 - val_mae: 0.1934\n",
      "Epoch 71/80\n",
      " - 92s - loss: 0.9615 - mse: 0.9217 - cosine_proximity: 0.2536 - mae: 0.7569 - val_loss: 0.0802 - val_mse: 0.0637 - val_cosine_proximity: 0.4850 - val_mae: 0.1934\n",
      "Epoch 72/80\n",
      " - 84s - loss: 0.9632 - mse: 0.9235 - cosine_proximity: 0.2532 - mae: 0.7573 - val_loss: 0.0802 - val_mse: 0.0637 - val_cosine_proximity: 0.4851 - val_mae: 0.1934\n",
      "Epoch 73/80\n",
      " - 81s - loss: 0.9629 - mse: 0.9230 - cosine_proximity: 0.2520 - mae: 0.7574 - val_loss: 0.0802 - val_mse: 0.0637 - val_cosine_proximity: 0.4852 - val_mae: 0.1934\n",
      "Epoch 74/80\n",
      " - 76s - loss: 0.9627 - mse: 0.9231 - cosine_proximity: 0.2540 - mae: 0.7575 - val_loss: 0.0802 - val_mse: 0.0637 - val_cosine_proximity: 0.4851 - val_mae: 0.1934\n",
      "Epoch 75/80\n",
      " - 78s - loss: 0.9556 - mse: 0.9160 - cosine_proximity: 0.2546 - mae: 0.7545 - val_loss: 0.0802 - val_mse: 0.0637 - val_cosine_proximity: 0.4852 - val_mae: 0.1934\n",
      "Epoch 76/80\n",
      " - 76s - loss: 0.9607 - mse: 0.9210 - cosine_proximity: 0.2531 - mae: 0.7564 - val_loss: 0.0802 - val_mse: 0.0637 - val_cosine_proximity: 0.4852 - val_mae: 0.1934\n",
      "Epoch 77/80\n",
      " - 76s - loss: 0.9599 - mse: 0.9204 - cosine_proximity: 0.2549 - mae: 0.7564 - val_loss: 0.0801 - val_mse: 0.0637 - val_cosine_proximity: 0.4852 - val_mae: 0.1934\n",
      "Epoch 78/80\n",
      " - 72s - loss: 0.9607 - mse: 0.9211 - cosine_proximity: 0.2538 - mae: 0.7565 - val_loss: 0.0801 - val_mse: 0.0637 - val_cosine_proximity: 0.4852 - val_mae: 0.1934\n",
      "Epoch 79/80\n",
      " - 102s - loss: 0.9575 - mse: 0.9180 - cosine_proximity: 0.2542 - mae: 0.7553 - val_loss: 0.0801 - val_mse: 0.0637 - val_cosine_proximity: 0.4851 - val_mae: 0.1934\n",
      "Epoch 80/80\n",
      " - 78s - loss: 0.9587 - mse: 0.9191 - cosine_proximity: 0.2541 - mae: 0.7558 - val_loss: 0.0801 - val_mse: 0.0637 - val_cosine_proximity: 0.4851 - val_mae: 0.1934\n"
     ]
    }
   ],
   "source": [
    "h=vision_model.fit_generator(train_generator,epochs=80,validation_data=test_generator,verbose=2,use_multiprocessing=False,callbacks=callbacks) #, steps_per_epoch=1200//64 , validation_steps=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vision_model.save_weights('./encoder_weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['val_loss', 'val_mse', 'val_cosine_proximity', 'val_mae', 'loss', 'mse', 'cosine_proximity', 'mae', 'lr'])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# list all data in history\n",
    "print(h.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(h.history['mse'])\n",
    "plt.plot(h.history['val_mse'])\n",
    "plt.title('model mse')\n",
    "plt.ylabel('mse')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(h.history['mae'])\n",
    "plt.plot(h.history['val_mae'])\n",
    "plt.title('model mae')\n",
    "plt.ylabel('mae')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(h.history['cosine_proximity'])\n",
    "plt.plot(h.history['val_cosine_proximity'])\n",
    "plt.title('model cosine proximity')\n",
    "plt.ylabel('cosine_promximity')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# # summarize history for loss\n",
    "plt.plot(h.history['loss'])\n",
    "plt.plot(h.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
